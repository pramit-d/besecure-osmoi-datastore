[
  {
    "type": "model",
    "name": "MathCoder",
    "description": "MathCoder is a family of models capable of generating code-based solutions for solving challenging math problems.",
    "created_date": {
      "explanation": "",
      "value": "2023-10-05"
    },
    "organization": "Shanghai AI Laboratory",
    "url": "https://arxiv.org/pdf/2310.03731.pdf",
    "model_card": "none",
    "modality": "text; text",
    "analysis": "Evaluated on GSM8K and the competition-level MATH dataset.",
    "size": "70B parameters (dense)",
    "dependencies": ["GPT-4", "LLaMA 2"],
    "training_emissions": "unknown",
    "training_time": "unknown",
    "training_hardware": "32 NVIDIA A800 80GB GPUs",
    "quality_control": "none",
    "access": "open",
    "license": {
      "explanation": "",
      "value": "unknown"
    },
    "intended_uses": "bridging the gap between natural language understanding and computational problem-solving",
    "prohibited_uses": "none",
    "monitoring": "none",
    "feedback": "none"
  },
  {
    "type": "model",
    "name": "Falcon-180B",
    "organization": "UAE Technology Innovation Institute",
    "description": "Falcon-180B is a 180B parameters causal decoder-only model built by TII and trained on 3,500B tokens of RefinedWeb enhanced with curated corpora.",
    "created_date": "2023-09-06",
    "url": "https://falconllm.tii.ae/falcon-models.html",
    "model_card": "https://huggingface.co/tiiuae/falcon-180B",
    "modality": "text; text",
    "analysis": "Falcon-180B outperforms LLaMA-2, StableLM, RedPajama, MPT on the Open LLM Leaderboard at https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.",
    "size": "180B parameters (dense)",
    "dependencies": ["RefinedWeb"],
    "training_emissions": "",
    "training_time": "9 months",
    "training_hardware": "4096 A100 40GB GPUs",
    "quality_control": "",
    "access": "open",
    "license": "unknown",
    "intended_uses": "",
    "prohibited_uses": "Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.",
    "monitoring": "None",
    "feedback": "https://huggingface.co/tiiuae/falcon-180b/discussions"
  },
  {
    "type": "model",
    "name": "resnet-18",
    "organization": "microsoft",
    "description": "ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.",
    "created_date": "2022-06-07",
    "url": "",
    "model_card": "https://huggingface.co/microsoft/resnet-18",
    "modality": "image; image",
    "analysis": "",
    "size": "11.7M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": "",
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "type": "model",
    "name": "bert-base-arabic-camelbert-ca-pos-glf",
    "organization": "CAMeL-Lab",
    "description": "CAMeLBERT-CA POS-GLF Model is a Gulf Arabic POS tagging model that was built by fine-tuning the CAMeLBERT-CA model. For the fine-tuning, we used the Gumar dataset.",
    "created_date": "2021-10-18",
    "url": "",
    "model_card": "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-ca-pos-glf",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": "",
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "type": "model",
    "name": "timeseries-anomaly-detection",
    "organization": "Pavithra Vijay",
    "description": "Reconstruction convolutional autoencoder model to detect anomalies in timeseries data. This uses the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics.",
    "created_date": "2022-06-03",
    "url": "https://huggingface.co/keras-io/timeseries-anomaly-detection",
    "model_card": "https://huggingface.co/keras-io/timeseries-anomaly-detection",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": "",
    "access": "open",
    "license": "unknown",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "type": "model",
    "name": "xgboost-example",
    "organization": "scikit-learn",
    "description": "This is an XGBoost model trained to predict daily alcohol consumption of students.",
    "created_date": "2023-01-20",
    "url": "None",
    "model_card": "https://huggingface.co/scikit-learn/xgboost-example",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": "",
    "access": "open",
    "license": "mit",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "type": "model",
    "name": "all-MiniLM-L6-v2",
    "organization": "sentence-transformers",
    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
    "created_date": "2021-08-31",
    "url": "None",
    "model_card": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
    "modality": "text; text",
    "analysis": "",
    "size": "1B",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": "",
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "as a sentence and short paragraph encoder",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "type": "model",
    "name": "paraphrase-multilingual-MiniLM-L12-v2",
    "organization": "sentence-transformers",
    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
    "created_date": "2021-06-09",
    "url": "None",
    "model_card": "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": "",
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "type": "model",
    "name": "bert-base-NER",
    "organization": "dslim",
    "description": "bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).",
    "created_date": "2020-12-12",
    "url": "None",
    "model_card": "https://huggingface.co/dslim/bert-base-NER",
    "modality": "",
    "analysis": "",
    "size": "108M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": "",
    "access": "open",
    "license": "mit",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "type": "model",
    "name": "vit-base-patch16-224",
    "organization": "google",
    "description": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch.",
    "created_date": "2021-03-24",
    "url": "None",
    "model_card": "https://huggingface.co/google/vit-base-patch16-224",
    "modality": "image; image",
    "analysis": "",
    "size": "86.6M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": "",
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "type": "model",
    "name": "resnet-50",
    "organization": "microsoft",
    "description": "VResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.",
    "created_date": "2022-06-07",
    "url": "None",
    "model_card": "https://huggingface.co/microsoft/resnet-50",
    "modality": "image; image",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": "",
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  }
]
